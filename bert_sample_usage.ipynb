{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "bert-sample-usage.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/honzas83/bio-bert/blob/main/bert_sample_usage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQT9duD7jYCw"
      },
      "source": [
        "# Example usage of BERT in protein classification\n",
        "\n",
        "## Required Python modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BdKR0bPo3pr"
      },
      "source": [
        "%%capture pip_install\n",
        "\n",
        "!pip install tensorflow-text \\\n",
        "             transformers==3.3.0 \\\n",
        "             pyyaml \\\n",
        "             sentencepiece \\\n",
        "             keras-bert==0.81.0 \\\n",
        "             keras-transformer==0.33.0 \\\n",
        "             pandas \\\n",
        "             scikit-learn==0.23 \\\n",
        "             biopython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T1_sFwcoi3j"
      },
      "source": [
        "## Model download\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdvxTcszlat7"
      },
      "source": [
        "!gdown --id 17FLvsbpjqR_SHAYY-S0YwAmjyLW2PYwx && unzip -u bert-pfam-10k.zip\n",
        "!gdown --id 10jnY335GcVon8EGqo5h5Kxh_gjQoxbJH && unzip -u SCOPe.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l6TOoNejYC_"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV72BD6ijYDC"
      },
      "source": [
        "import keras_bert\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "\n",
        "import sentencepiece \n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import random\n",
        "from Bio import SeqIO\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras import layers\n",
        "from keras.layers import Input, Dense, TimeDistributed, Bidirectional, LSTM, Concatenate, Conv1D, Dropout, Dot, Lambda, GlobalAvgPool1D, Add, GaussianNoise, Flatten\n",
        "from keras.models import Model, load_model\n",
        "from keras_bert.layers import Extract, MaskedGlobalMaxPool1D\n",
        "from keras.regularizers import l1\n",
        "from keras_bert import gelu\n",
        "from keras_position_wise_feed_forward import FeedForward\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, average_precision_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4zMepUBjYDD"
      },
      "source": [
        "### Experiment configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W878yIuHjYDE"
      },
      "source": [
        "# BERT pretrained model\n",
        "CHECKPOINT_PATH = './bert-pfam-10k/'\n",
        "CHECKPOINT_NAME = 'model.ckpt-1500000'\n",
        "BERT_CONFIG = 'bert_config.json'\n",
        "\n",
        "# Tokenization model for SentencePieces\n",
        "SPM_MODEL = \"./bert-pfam-10k/pfam.model\"\n",
        "\n",
        "# Load 12 layers from BERT\n",
        "LAYER_NUM = 12\n",
        "\n",
        "# Fine-tune the self-attention layers in FINE_TUNE\n",
        "#FINE_TUNE = [10, 11, 12]\n",
        "FINE_TUNE = []\n",
        "\n",
        "# Use True to increase number of trainable parameters in the BERT\n",
        "# Requires: keras-bert==0.81.0 keras-transformer==0.33.0\n",
        "USE_ADAPTER = False\n",
        "#USE_ADAPTER = True   \n",
        "\n",
        "# Output pooling type extract/max/kmax\n",
        "POOL_TYPE = \"kmax\"\n",
        "POOL_K = 4  # k-max pooling parameter\n",
        "\n",
        "# Specification of SCOPe classes used in classification\n",
        "# See: https://scop.berkeley.edu/ver=2.07\n",
        "TARGETS = \"abcd\"\n",
        "#TARGETS = \"abcdefg\"\n",
        "\n",
        "# Experimental data \n",
        "# Download from: http://bergerlab-downloads.csail.mit.edu/bepler-protein-sequence-embeddings-from-structure-iclr2019/scope.tar.gz\n",
        "# See also: https://github.com/tbepler/protein-sequence-embedding-iclr2019#data-sets\n",
        "TRAIN_DATA = \"./SCOPe/astral-scopedom-seqres-gd-sel-gs-bib-95-2.06.train.train.fa\"\n",
        "DEV_DATA = \"./SCOPe/astral-scopedom-seqres-gd-sel-gs-bib-95-2.06.train.dev.fa\"\n",
        "TEST_DATA = \"./SCOPe/astral-scopedom-seqres-gd-sel-gs-bib-95-2.06.test.fa\"\n",
        "TEST_DATA_NEW = \"./SCOPe/astral-scopedom-seqres-gd-sel-gs-bib-95-2.07-new.fa\"\n",
        "\n",
        "# Maximum number of SentencePieces per example\n",
        "MAX_LEN = 128\n",
        "MAX_LEN1 = (MAX_LEN-2)  # Reserve two slots for [CLS] and [SEP] tokens\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 32   # Limited by the size of GPU\n",
        "LR_START = 1e-3   # Initial learning rate, uses a simple Sqrt decay\n",
        "EPOCHS = 3       # Number of training epochs\n",
        "\n",
        "# SentencePieces allow sampling of different tokenizations\n",
        "# Use PREDICT_AVG=1 to disable averaging and use the non-sampled tokenization only\n",
        "PREDICT_AVG = 5\n",
        "\n",
        "# Save the resulting model into this file\n",
        "SAVE_MODEL_FN = \"bert-sample-usage.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIK07r4_jYDF"
      },
      "source": [
        "### Custom class for K-max pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjrio1-njYDF"
      },
      "source": [
        "class KMaxPooling(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
        "    TensorFlow backend.\n",
        "    \"\"\"\n",
        "    def __init__(self, k=1, axis=1, **kwargs):\n",
        "        super(KMaxPooling, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.input_spec = layers.InputSpec(ndim=3)\n",
        "        self.k = k\n",
        "\n",
        "        assert axis in [1,2],  'expected dimensions (samples, filters, convolved_values),\\\n",
        "                   cannot fold along samples dimension or axis not in list [1,2]'\n",
        "        self.axis = axis\n",
        "\n",
        "        # need to switch the axis with the last elemnet\n",
        "        # to perform transpose for tok k elements since top_k works in last axis\n",
        "        self.transpose_perm = [0,1,2] #default\n",
        "        self.transpose_perm[self.axis] = 2\n",
        "        self.transpose_perm[2] = self.axis\n",
        "    \n",
        "    def get_config(self):\n",
        "        return {\"k\": self.k, \"axis\": self.axis}\n",
        "   \n",
        "    def build(self, input_shape):\n",
        "        super(KMaxPooling, self).build(input_shape)\n",
        "        self._my_output_shape = [i for i in input_shape]\n",
        "        self._my_output_shape[0] = -1\n",
        "        self._my_output_shape[self.axis] = self.k\n",
        "        self._my_output_shape = tf.convert_to_tensor(self._my_output_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        # swap sequence dimension to get top k elements along axis=1\n",
        "        transposed_for_topk = tf.transpose(x, perm=self.transpose_perm)\n",
        "\n",
        "        # extract top_k, returns two tensors [values, indices]\n",
        "        top_k_vals, top_k_indices = tf.math.top_k(transposed_for_topk,\n",
        "                                                  k=self.k, sorted=True,\n",
        "                                                  name=None)\n",
        "        # maintain the order of values as in the paper\n",
        "        # sort indices\n",
        "        sorted_top_k_ind = tf.sort(top_k_indices)\n",
        "        flatten_seq = tf.reshape(transposed_for_topk, (-1,))\n",
        "        shape_seq = tf.shape(transposed_for_topk)\n",
        "        len_seq = tf.shape(flatten_seq)[0]\n",
        "        indices_seq = tf.range(len_seq)\n",
        "        indices_seq = tf.reshape(indices_seq, shape_seq)\n",
        "        indices_gather = tf.gather(indices_seq, 0, axis=-1)\n",
        "        indices_sum = tf.expand_dims(indices_gather, axis=-1)\n",
        "        sorted_top_k_ind += indices_sum\n",
        "        k_max_out = tf.gather(flatten_seq, sorted_top_k_ind)\n",
        "        # return back to normal dimension but now sequence dimension has only k elements\n",
        "        # performing another transpose will get the tensor back to its original shape\n",
        "        # but will have k as its axis_1 size\n",
        "        transposed_back = tf.transpose(k_max_out, perm=self.transpose_perm)\n",
        "\n",
        "        \n",
        "        return tf.reshape(transposed_back, self._my_output_shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyzGCipCjYDH"
      },
      "source": [
        "### Pre-trained BERT loading\n",
        "\n",
        "Loads the pre-trained BERT model into Keras model.\n",
        "\n",
        "See also: https://github.com/CyberZHG/keras-bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WFJ2dxJCjYDW"
      },
      "source": [
        "config_path = os.path.join(CHECKPOINT_PATH, BERT_CONFIG)\n",
        "model_path = os.path.join(CHECKPOINT_PATH, CHECKPOINT_NAME)\n",
        "trainable = (['Encoder-{}-MultiHeadSelfAttention-Adapter'.format(i + 1) for i in range(LAYER_NUM)] +\n",
        "    ['Encoder-{}-FeedForward-Adapter'.format(i + 1) for i in range(LAYER_NUM)] +\n",
        "    ['Encoder-{}-MultiHeadSelfAttention-Norm'.format(i + 1) for i in range(LAYER_NUM)] +\n",
        "    ['Encoder-{}-FeedForward-Norm'.format(i + 1) for i in range(LAYER_NUM)] +\n",
        "    ['Encoder-{}-MultiHeadSelfAttention'.format(i) for i in FINE_TUNE])\n",
        "\n",
        "\n",
        "bert_model = load_trained_model_from_checkpoint(\n",
        "    config_path,\n",
        "    model_path,\n",
        "    seq_len=MAX_LEN,\n",
        "    training=False,\n",
        "    trainable=trainable,\n",
        "    **({} if USE_ADAPTER else {\"use_adapter\": True})\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jJ-vpl4jYDW"
      },
      "source": [
        "### SentencePiece model initialization\n",
        "\n",
        "Loads the SentencePiece tokenizer from file and defines some helper functions.\n",
        "\n",
        "See also: https://github.com/google/sentencepiece/blob/master/python/README.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ZXOKuUjQjYDX"
      },
      "source": [
        "spm = sentencepiece.SentencePieceProcessor(SPM_MODEL)\n",
        "\n",
        "def encode(seq, enable_sampling=True):\n",
        "    if enable_sampling:\n",
        "        ret = spm.encode(seq.upper(), out_type=str, enable_sampling=True, alpha=0.2, nbest_size=-1)\n",
        "    else:\n",
        "        ret = spm.encode(seq.upper(), out_type=str)\n",
        "    ret = [i.lstrip(\"▁\") for i in ret]\n",
        "    ret = [i for i in ret if i]\n",
        "    return ret\n",
        "\n",
        "UNK = spm.PieceToId(\"[UNK]\")\n",
        "\n",
        "def PieceToId(piece):\n",
        "    ret = spm.PieceToId(piece)\n",
        "    if ret == 0:\n",
        "        return UNK\n",
        "    else:\n",
        "        return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on7gn0k3jYDX"
      },
      "source": [
        "### Batch vectorization\n",
        "\n",
        "Vectorizes the input batch, which is a list of (sequence, target_class) pairs.\n",
        "\n",
        "Returns the tuple ([tokens, segments], targets) where:\n",
        " - tokens is an array the shape (batch_size, MAX_LEN) encoding the SentencePieces\n",
        " - segments is and array of zeros with the same shape\n",
        " - targets are vectorized softmax target values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E1g9FaejYDY"
      },
      "source": [
        "def vectorize_batch(batch, enable_sampling=True):\n",
        "\n",
        "    tokens = []\n",
        "    segments = []\n",
        "    targets = []\n",
        "    for seqA, y in batch:\n",
        "        if y not in TARGETS:\n",
        "            continue\n",
        "            \n",
        "        tokens1 = []\n",
        "        tokens1.append(PieceToId(\"[CLS]\"))\n",
        "        encoded = encode(seqA, enable_sampling)\n",
        "        if len(encoded) > MAX_LEN1:\n",
        "            if random.random() < 0.5:\n",
        "                encoded = encoded[:MAX_LEN1]\n",
        "            else:\n",
        "                encoded = encoded[-MAX_LEN1:]\n",
        "        tokens1.extend([PieceToId(i) for i in encoded])\n",
        "        tokens1.append(PieceToId(\"[SEP]\"))\n",
        "         \n",
        "        segments1 = [0]*len(tokens1)\n",
        "\n",
        "        assert len(tokens1) == len(segments1)\n",
        "        while len(tokens1) < MAX_LEN:\n",
        "            tokens1.append(0)\n",
        "            segments1.append(0)\n",
        "\n",
        "        Y = [0]*len(TARGETS)\n",
        "        Y[TARGETS.index(y)] = 1\n",
        "            \n",
        "        tokens.append(tokens1)\n",
        "        segments.append(segments1)\n",
        "        targets.append(Y)\n",
        "    \n",
        "    tokens = np.array(tokens, dtype=np.int32)\n",
        "    segments = np.array(segments, dtype=np.int32)\n",
        "    targets = np.array(targets)\n",
        "    return [tokens, segments], targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEl8JXBJjYDY"
      },
      "source": [
        "### Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRLrCVtpjYDY"
      },
      "source": [
        "input_tokens = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "input_segments = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "\n",
        "bert_out = bert_model([input_tokens, input_segments])\n",
        "\n",
        "bert_out = Dropout(0.25, noise_shape=(None, 128, 1))(bert_out)\n",
        "\n",
        "dense_out = TimeDistributed(Dense(256, activation=\"relu\"))(bert_out)\n",
        "\n",
        "if POOL_TYPE == \"extract\":\n",
        "    pool_out = Extract(index=0)(dense_out)\n",
        "elif POOL_TYPE == \"max\":\n",
        "    pool_out = MaskedGlobalMaxPool1D()(dense_out)\n",
        "elif POOL_TYPE == \"kmax\":    \n",
        "    pool_out = KMaxPooling(k=POOL_K)(dense_out)\n",
        "    pool_out = Flatten()(pool_out)\n",
        "\n",
        "out = Dense(len(TARGETS))(pool_out)\n",
        "out = keras.layers.Activation(\"softmax\")(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04GyfkftjYDY"
      },
      "source": [
        "model = Model(inputs=[input_tokens, input_segments],\n",
        "              outputs=[out])\n",
        "model.compile(optimizer=\"adam\", loss=[\"categorical_crossentropy\"], metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNt36S7XjYDY"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_xBUueCjYDZ"
      },
      "source": [
        "### Dataset loading functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHqKdeIajYDa"
      },
      "source": [
        "def iter_training_sample(fn):\n",
        "    ret = []\n",
        "    for seq_record in SeqIO.parse(fn, \"fasta\"):\n",
        "        struct = seq_record.description.split()[1]\n",
        "        seq = str(seq_record.seq)\n",
        "        ret.append((seq.upper(), struct[0]))\n",
        "    random.seed(fn)\n",
        "    random.shuffle(ret)\n",
        "    return ret\n",
        "\n",
        "def train_gen(data, batch_size, shuffle=True, enable_sampling=True):\n",
        "    while True:\n",
        "        data_it = data[:]\n",
        "        if shuffle:\n",
        "            random.shuffle(data_it)\n",
        "        for idx0 in range(0, len(data_it), batch_size):\n",
        "            batch = data_it[idx0:idx0+batch_size]\n",
        "            yield vectorize_batch(batch, enable_sampling=enable_sampling)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s9AAEzGjYDa"
      },
      "source": [
        "train_data = iter_training_sample(TRAIN_DATA)\n",
        "train_iter = train_gen(train_data, BATCH_SIZE)\n",
        "TRAIN_STEPS = len(train_data) // BATCH_SIZE\n",
        "\n",
        "dev_data = iter_training_sample(DEV_DATA)\n",
        "dev_iter = train_gen(dev_data, BATCH_SIZE, enable_sampling=False)\n",
        "DEV_STEPS = len(dev_data) // BATCH_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kphMdrFfjYDc"
      },
      "source": [
        "### Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVqll_HejYDd"
      },
      "source": [
        "callbacks = [LearningRateScheduler(lambda n: LR_START*1/((n+1)**0.5), verbose=1)]\n",
        "\n",
        "model.fit(train_iter, steps_per_epoch=TRAIN_STEPS, epochs=EPOCHS,\n",
        "          validation_data=dev_iter,\n",
        "          validation_steps=DEV_STEPS,\n",
        "          callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzqdr3-LjYDe"
      },
      "source": [
        "### Predictions on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw1KH30ojYDe"
      },
      "source": [
        "test_data = iter_training_sample(TEST_DATA)\n",
        "\n",
        "pred_Y = 0\n",
        "for i in range(PREDICT_AVG):\n",
        "    test_X, test_Y = next(train_gen(test_data, len(test_data), shuffle=False, enable_sampling=(i!=0)))\n",
        "    pred_Y = pred_Y + model.predict(test_X, batch_size=128, verbose=True)\n",
        "\n",
        "test_Y1 = test_Y.argmax(axis=1)\n",
        "pred_Y1 = pred_Y.argmax(axis=1)\n",
        "print(\"Model accuracy\", accuracy_score(test_Y1, pred_Y1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jQjpAyYzjYDe"
      },
      "source": [
        "print(\"Confusion matrix\")\n",
        "pd.DataFrame(confusion_matrix(test_Y1, pred_Y1),\n",
        "             columns=list(TARGETS),\n",
        "             index=list(TARGETS)).round(2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eivpBhUjYDe"
      },
      "source": [
        "test_data = iter_training_sample(TEST_DATA_NEW)\n",
        "\n",
        "pred_Y = 0\n",
        "for i in range(PREDICT_AVG):\n",
        "    test_X, test_Y = next(train_gen(test_data, len(test_data), shuffle=False, enable_sampling=(i!=0)))\n",
        "    pred_Y = pred_Y + model.predict(test_X, batch_size=128, verbose=True)\n",
        "\n",
        "test_Y1 = test_Y.argmax(axis=1)\n",
        "pred_Y1 = pred_Y.argmax(axis=1)\n",
        "print(\"Model accuracy\", accuracy_score(test_Y1, pred_Y1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc0XQU45jYDf"
      },
      "source": [
        "model.save(SAVE_MODEL_FN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDNYbAUlB4jw"
      },
      "source": [
        "## Copy out the resulting model into Google Drive\n",
        "\n",
        "Clink on the displayed link and authorize the app to access your google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXQumnWzCAzg"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ofvWTI2HBFe"
      },
      "source": [
        "Modify the target directory according to your needs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo4sda_dCJB7"
      },
      "source": [
        "!cp $SAVE_MODEL_FN gdrive/MyDrive/Models/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}