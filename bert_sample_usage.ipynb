{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "bert-sample-usage.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/honzas83/bio-bert/blob/main/bert_sample_usage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQT9duD7jYCw"
      },
      "source": [
        "# Example usage of BERT in protein classification\n",
        "\n",
        "## Required Python modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BdKR0bPo3pr",
        "outputId": "5cc9e164-bead-4b5e-8bed-20e24c7129ea"
      },
      "source": [
        "%%capture pip_install\n",
        "\n",
        "!pip install tensorflow-text \\\n",
        "             transformers==3.3.0 \\\n",
        "             pyyaml \\\n",
        "             sentencepiece \\\n",
        "             keras-bert==0.81.0 \\\n",
        "             keras-transformer==0.33.0 \\\n",
        "             pandas \\\n",
        "             scikit-learn==0.23 \\\n",
        "             biopython"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/86/22ad798f94d564c3e423758b60ddd3689e83ad629b3f31ff2ae45a6e3eed/tensorflow_text-2.4.3-cp36-cp36m-manylinux1_x86_64.whl (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 8.7MB/s \n",
            "\u001b[?25hCollecting transformers==3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/fc/18e56e5b1093052bacf6750442410423f3d9785d14ce4f54ab2ac6b112a6/transformers-3.3.0-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 49.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 55.8MB/s \n",
            "\u001b[?25hCollecting keras-bert==0.81.0\n",
            "  Downloading https://files.pythonhosted.org/packages/2c/0f/cdc886c1018943ea62d3209bc964413d5aa9d0eb7e493abd8545be679294/keras-bert-0.81.0.tar.gz\n",
            "Collecting keras-transformer==0.33.0\n",
            "  Downloading https://files.pythonhosted.org/packages/22/b9/9040ec948ef895e71df6bee505a1f7e1c99ffedb409cb6eb329f04ece6e0/keras-transformer-0.33.0.tar.gz\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
            "Collecting scikit-learn==0.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/cd/af7469eb9d2b2ba428206273585f0272b6094ae915aed0b1ca99eace56df/scikit_learn-0.23.0-cp36-cp36m-manylinux1_x86_64.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 46.0MB/s \n",
            "\u001b[?25hCollecting biopython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/02/8b606c4aa92ff61b5eda71d23b499ab1de57d5e818be33f77b01a6f435a8/biopython-1.78-cp36-cp36m-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 57.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text) (0.11.0)\n",
            "Requirement already satisfied: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text) (2.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 53.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.0) (1.19.5)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert==0.81.0) (2.4.3)\n",
            "Collecting keras-pos-embd>=0.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.22.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23) (1.0.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23) (1.4.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub>=0.8.0->tensorflow-text) (3.12.4)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (0.36.2)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.32.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (0.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (2.4.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (2.10.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (0.3.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.12)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (2.4.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.0) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.3.0) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.0) (7.1.2)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-hub>=0.8.0->tensorflow-text) (53.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (1.24.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (3.3.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (4.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text) (3.4.0)\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, sacremoses, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.81.0-cp36-none-any.whl size=37913 sha256=e4d89a61d9fa6b122d301e10ac953237e24b388592cf3bfaa499b9e65c852f7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/27/da/ffc2d573aa48b87440ec4f98bc7c992e3a2d899edb2d22ef9e\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.33.0-cp36-none-any.whl size=13261 sha256=2c948b3fff6e4df49d830fca62440e64791e6bc958c5ff44ef194d37a9d4c2d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/98/13/a28402939e1d48edd8704e6b02f223795af4a706815f4bf6d8\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=24a137b71a1303318f9ff89217400f743d7b352c8a92950cb3fb857e9a89d35b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7553 sha256=279dfabcbceb7a15683c2d9e975dd83f376f4152ab251dea6a6de0768d48f9e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15612 sha256=7d32815c204a217f8cfc1b151eecef0574d093f2b1b43790a194ad9773332700\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5267 sha256=db70fbbf76fe3d830b5ca799da70c0b6594420a2b1588fabc3ea18b8828a2b9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5625 sha256=35e4294473740690435e8dd2f74da1c2087ba61f1163f914d6893743605f0baa\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4559 sha256=ea94ae075b8bd0b34005cc9b774974fd19c42ee9feb848a9aaf78a81a11aa42f\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=fbbcd82251169af57da26fcece5dd6eedaa8586ea93ffbb5cbc669feec5c54a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-bert keras-transformer sacremoses keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: tensorflow-text, sacremoses, tokenizers, sentencepiece, transformers, keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert, threadpoolctl, scikit-learn, biopython\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed biopython-1.78 keras-bert-0.81.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.33.0 sacremoses-0.0.43 scikit-learn-0.23.0 sentencepiece-0.1.95 tensorflow-text-2.4.3 threadpoolctl-2.1.0 tokenizers-0.8.1rc2 transformers-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T1_sFwcoi3j"
      },
      "source": [
        "## Model download\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdvxTcszlat7",
        "outputId": "fce0e120-9b98-4791-8c21-f40d1d2c9f5b"
      },
      "source": [
        "!gdown --id 17FLvsbpjqR_SHAYY-S0YwAmjyLW2PYwx && unzip -u bert-pfam-10k.zip\n",
        "!gdown --id 10jnY335GcVon8EGqo5h5Kxh_gjQoxbJH && unzip -u SCOPe.zip\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17FLvsbpjqR_SHAYY-S0YwAmjyLW2PYwx\n",
            "To: /content/bert-pfam-10k.zip\n",
            "1.04GB [00:10, 98.6MB/s]\n",
            "Archive:  bert-pfam-10k.zip\n",
            "   creating: bert-pfam-10k/\n",
            "  inflating: bert-pfam-10k/checkpoint  \n",
            "  inflating: bert-pfam-10k/pfam.vocab  \n",
            "  inflating: bert-pfam-10k/model.ckpt-1500000.index  \n",
            "  inflating: bert-pfam-10k/model.ckpt-1500000.data-00000-of-00001  \n",
            "  inflating: bert-pfam-10k/bert_config.json  \n",
            "  inflating: bert-pfam-10k/pfam.model  \n",
            "  inflating: bert-pfam-10k/model.ckpt-1500000.meta  \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10jnY335GcVon8EGqo5h5Kxh_gjQoxbJH\n",
            "To: /content/SCOPe.zip\n",
            "14.2MB [00:00, 45.2MB/s]\n",
            "Archive:  SCOPe.zip\n",
            "   creating: SCOPe/\n",
            "  inflating: SCOPe/astral-scopedom-seqres-gd-sel-gs-bib-95-2.06.test.fa  \n",
            "  inflating: SCOPe/astral-scopedom-seqres-gd-sel-gs-bib-95-2.07.fa  \n",
            "  inflating: SCOPe/astral-scopedom-seqres-gd-sel-gs-bib-95-2.06.fa  \n",
            "  inflating: SCOPe/astral-scopedom-seqres-gd-sel-gs-bib-95-2.06.train.train.fa  \n",
            "  inflating: SCOPe/astral-scopedom-seqres-gd-sel-gs-bib-95-2.06.train.dev.fa  \n",
            "  inflating: SCOPe/astral-scopedom-seqres-gd-sel-gs-bib-95-2.07-new.fa  \n",
            "  inflating: SCOPe/astral-scopedom-seqres-gd-sel-gs-bib-95-2.06.train.fa  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l6TOoNejYC_"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV72BD6ijYDC"
      },
      "source": [
        "import keras_bert\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "\n",
        "import sentencepiece \n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import random\n",
        "from Bio import SeqIO\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras import layers\n",
        "from keras.layers import Input, Dense, TimeDistributed, Bidirectional, LSTM, Concatenate, Conv1D, Dropout, Dot, Lambda, GlobalAvgPool1D, Add, GaussianNoise, Flatten\n",
        "from keras.models import Model, load_model\n",
        "from keras_bert.layers import Extract, MaskedGlobalMaxPool1D\n",
        "from keras.regularizers import l1\n",
        "from keras_bert import gelu\n",
        "from keras_position_wise_feed_forward import FeedForward\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, average_precision_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4zMepUBjYDD"
      },
      "source": [
        "### Experiment configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W878yIuHjYDE"
      },
      "source": [
        "# BERT pretrained model\n",
        "CHECKPOINT_PATH = './bert-pfam-10k/'\n",
        "CHECKPOINT_NAME = 'model.ckpt-1500000'\n",
        "BERT_CONFIG = 'bert_config.json'\n",
        "\n",
        "# Tokenization model for SentencePieces\n",
        "SPM_MODEL = \"./bert-pfam-10k/pfam.model\"\n",
        "\n",
        "# Load 12 layers from BERT\n",
        "LAYER_NUM = 12\n",
        "\n",
        "# Fine-tune the self-attention layers in FINE_TUNE\n",
        "#FINE_TUNE = [10, 11, 12]\n",
        "FINE_TUNE = []\n",
        "\n",
        "# Use True to increase number of trainable parameters in the BERT\n",
        "# Requires: keras-bert==0.81.0 keras-transformer==0.33.0\n",
        "USE_ADAPTER = False\n",
        "#USE_ADAPTER = True   \n",
        "\n",
        "# Output pooling type extract/max/kmax\n",
        "POOL_TYPE = \"kmax\"\n",
        "POOL_K = 4  # k-max pooling parameter\n",
        "\n",
        "# Specification of SCOPe classes used in classification\n",
        "# See: https://scop.berkeley.edu/ver=2.07\n",
        "TARGETS = \"abcd\"\n",
        "#TARGETS = \"abcdefg\"\n",
        "\n",
        "# Experimental data \n",
        "# Download from: http://bergerlab-downloads.csail.mit.edu/bepler-protein-sequence-embeddings-from-structure-iclr2019/scope.tar.gz\n",
        "# See also: https://github.com/tbepler/protein-sequence-embedding-iclr2019#data-sets\n",
        "TRAIN_DATA = \"./SCOPe/astral-scopedom-seqres-gd-sel-gs-bib-95-2.06.train.train.fa\"\n",
        "DEV_DATA = \"./SCOPe/astral-scopedom-seqres-gd-sel-gs-bib-95-2.06.train.dev.fa\"\n",
        "TEST_DATA = \"./SCOPe/astral-scopedom-seqres-gd-sel-gs-bib-95-2.06.test.fa\"\n",
        "TEST_DATA_NEW = \"./SCOPe/astral-scopedom-seqres-gd-sel-gs-bib-95-2.07-new.fa\"\n",
        "\n",
        "# Maximum number of SentencePieces per example\n",
        "MAX_LEN = 128\n",
        "MAX_LEN1 = (MAX_LEN-2)  # Reserve two slots for [CLS] and [SEP] tokens\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 32   # Limited by the size of GPU\n",
        "LR_START = 1e-3   # Initial learning rate, uses a simple Sqrt decay\n",
        "EPOCHS = 3       # Number of training epochs\n",
        "\n",
        "# SentencePieces allow sampling of different tokenizations\n",
        "# Use PREDICT_AVG=1 to disable averaging and use the non-sampled tokenization only\n",
        "PREDICT_AVG = 5\n",
        "\n",
        "# Save the resulting model into this file\n",
        "SAVE_MODEL_FN = \"bert-sample-usage.hdf5\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIK07r4_jYDF"
      },
      "source": [
        "### Custom class for K-max pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjrio1-njYDF"
      },
      "source": [
        "class KMaxPooling(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
        "    TensorFlow backend.\n",
        "    \"\"\"\n",
        "    def __init__(self, k=1, axis=1, **kwargs):\n",
        "        super(KMaxPooling, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.input_spec = layers.InputSpec(ndim=3)\n",
        "        self.k = k\n",
        "\n",
        "        assert axis in [1,2],  'expected dimensions (samples, filters, convolved_values),\\\n",
        "                   cannot fold along samples dimension or axis not in list [1,2]'\n",
        "        self.axis = axis\n",
        "\n",
        "        # need to switch the axis with the last elemnet\n",
        "        # to perform transpose for tok k elements since top_k works in last axis\n",
        "        self.transpose_perm = [0,1,2] #default\n",
        "        self.transpose_perm[self.axis] = 2\n",
        "        self.transpose_perm[2] = self.axis\n",
        "    \n",
        "    def get_config(self):\n",
        "        return {\"k\": self.k, \"axis\": self.axis}\n",
        "   \n",
        "    def build(self, input_shape):\n",
        "        super(KMaxPooling, self).build(input_shape)\n",
        "        self._my_output_shape = [i for i in input_shape]\n",
        "        self._my_output_shape[0] = -1\n",
        "        self._my_output_shape[self.axis] = self.k\n",
        "        self._my_output_shape = tf.convert_to_tensor(self._my_output_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        # swap sequence dimension to get top k elements along axis=1\n",
        "        transposed_for_topk = tf.transpose(x, perm=self.transpose_perm)\n",
        "\n",
        "        # extract top_k, returns two tensors [values, indices]\n",
        "        top_k_vals, top_k_indices = tf.math.top_k(transposed_for_topk,\n",
        "                                                  k=self.k, sorted=True,\n",
        "                                                  name=None)\n",
        "        # maintain the order of values as in the paper\n",
        "        # sort indices\n",
        "        sorted_top_k_ind = tf.sort(top_k_indices)\n",
        "        flatten_seq = tf.reshape(transposed_for_topk, (-1,))\n",
        "        shape_seq = tf.shape(transposed_for_topk)\n",
        "        len_seq = tf.shape(flatten_seq)[0]\n",
        "        indices_seq = tf.range(len_seq)\n",
        "        indices_seq = tf.reshape(indices_seq, shape_seq)\n",
        "        indices_gather = tf.gather(indices_seq, 0, axis=-1)\n",
        "        indices_sum = tf.expand_dims(indices_gather, axis=-1)\n",
        "        sorted_top_k_ind += indices_sum\n",
        "        k_max_out = tf.gather(flatten_seq, sorted_top_k_ind)\n",
        "        # return back to normal dimension but now sequence dimension has only k elements\n",
        "        # performing another transpose will get the tensor back to its original shape\n",
        "        # but will have k as its axis_1 size\n",
        "        transposed_back = tf.transpose(k_max_out, perm=self.transpose_perm)\n",
        "\n",
        "        \n",
        "        return tf.reshape(transposed_back, self._my_output_shape)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyzGCipCjYDH"
      },
      "source": [
        "### Pre-trained BERT loading\n",
        "\n",
        "Loads the pre-trained BERT model into Keras model.\n",
        "\n",
        "See also: https://github.com/CyberZHG/keras-bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WFJ2dxJCjYDW"
      },
      "source": [
        "config_path = os.path.join(CHECKPOINT_PATH, BERT_CONFIG)\n",
        "model_path = os.path.join(CHECKPOINT_PATH, CHECKPOINT_NAME)\n",
        "trainable = (['Encoder-{}-MultiHeadSelfAttention-Adapter'.format(i + 1) for i in range(LAYER_NUM)] +\n",
        "    ['Encoder-{}-FeedForward-Adapter'.format(i + 1) for i in range(LAYER_NUM)] +\n",
        "    ['Encoder-{}-MultiHeadSelfAttention-Norm'.format(i + 1) for i in range(LAYER_NUM)] +\n",
        "    ['Encoder-{}-FeedForward-Norm'.format(i + 1) for i in range(LAYER_NUM)] +\n",
        "    ['Encoder-{}-MultiHeadSelfAttention'.format(i) for i in FINE_TUNE])\n",
        "\n",
        "\n",
        "bert_model = load_trained_model_from_checkpoint(\n",
        "    config_path,\n",
        "    model_path,\n",
        "    seq_len=MAX_LEN,\n",
        "    training=False,\n",
        "    trainable=trainable,\n",
        "    **({} if USE_ADAPTER else {\"use_adapter\": True})\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jJ-vpl4jYDW"
      },
      "source": [
        "### SentencePiece model initialization\n",
        "\n",
        "Loads the SentencePiece tokenizer from file and defines some helper functions.\n",
        "\n",
        "See also: https://github.com/google/sentencepiece/blob/master/python/README.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ZXOKuUjQjYDX"
      },
      "source": [
        "spm = sentencepiece.SentencePieceProcessor(SPM_MODEL)\n",
        "\n",
        "def encode(seq, enable_sampling=True):\n",
        "    if enable_sampling:\n",
        "        ret = spm.encode(seq.upper(), out_type=str, enable_sampling=True, alpha=0.2, nbest_size=-1)\n",
        "    else:\n",
        "        ret = spm.encode(seq.upper(), out_type=str)\n",
        "    ret = [i.lstrip(\"▁\") for i in ret]\n",
        "    ret = [i for i in ret if i]\n",
        "    return ret\n",
        "\n",
        "UNK = spm.PieceToId(\"[UNK]\")\n",
        "\n",
        "def PieceToId(piece):\n",
        "    ret = spm.PieceToId(piece)\n",
        "    if ret == 0:\n",
        "        return UNK\n",
        "    else:\n",
        "        return ret"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on7gn0k3jYDX"
      },
      "source": [
        "### Batch vectorization\n",
        "\n",
        "Vectorizes the input batch, which is a list of (sequence, target_class) pairs.\n",
        "\n",
        "Returns the tuple ([tokens, segments], targets) where:\n",
        " - tokens is an array the shape (batch_size, MAX_LEN) encoding the SentencePieces\n",
        " - segments is and array of zeros with the same shape\n",
        " - targets are vectorized softmax target values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E1g9FaejYDY"
      },
      "source": [
        "def vectorize_batch(batch, enable_sampling=True):\n",
        "\n",
        "    tokens = []\n",
        "    segments = []\n",
        "    targets = []\n",
        "    for seqA, y in batch:\n",
        "        if y not in TARGETS:\n",
        "            continue\n",
        "            \n",
        "        tokens1 = []\n",
        "        tokens1.append(PieceToId(\"[CLS]\"))\n",
        "        encoded = encode(seqA, enable_sampling)\n",
        "        if len(encoded) > MAX_LEN1:\n",
        "            if random.random() < 0.5:\n",
        "                encoded = encoded[:MAX_LEN1]\n",
        "            else:\n",
        "                encoded = encoded[-MAX_LEN1:]\n",
        "        tokens1.extend([PieceToId(i) for i in encoded])\n",
        "        tokens1.append(PieceToId(\"[SEP]\"))\n",
        "         \n",
        "        segments1 = [0]*len(tokens1)\n",
        "\n",
        "        assert len(tokens1) == len(segments1)\n",
        "        while len(tokens1) < MAX_LEN:\n",
        "            tokens1.append(0)\n",
        "            segments1.append(0)\n",
        "\n",
        "        Y = [0]*len(TARGETS)\n",
        "        Y[TARGETS.index(y)] = 1\n",
        "            \n",
        "        tokens.append(tokens1)\n",
        "        segments.append(segments1)\n",
        "        targets.append(Y)\n",
        "    \n",
        "    tokens = np.array(tokens, dtype=np.int32)\n",
        "    segments = np.array(segments, dtype=np.int32)\n",
        "    targets = np.array(targets)\n",
        "    return [tokens, segments], targets"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEl8JXBJjYDY"
      },
      "source": [
        "### Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRLrCVtpjYDY"
      },
      "source": [
        "input_tokens = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "input_segments = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "\n",
        "bert_out = bert_model([input_tokens, input_segments])\n",
        "\n",
        "bert_out = Dropout(0.25, noise_shape=(None, 128, 1))(bert_out)\n",
        "\n",
        "dense_out = TimeDistributed(Dense(256, activation=\"relu\"))(bert_out)\n",
        "\n",
        "if POOL_TYPE == \"extract\":\n",
        "    pool_out = Extract(index=0)(dense_out)\n",
        "elif POOL_TYPE == \"max\":\n",
        "    pool_out = MaskedGlobalMaxPool1D()(dense_out)\n",
        "elif POOL_TYPE == \"kmax\":    \n",
        "    pool_out = KMaxPooling(k=POOL_K)(dense_out)\n",
        "    pool_out = Flatten()(pool_out)\n",
        "\n",
        "out = Dense(len(TARGETS))(pool_out)\n",
        "out = keras.layers.Activation(\"softmax\")(out)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04GyfkftjYDY"
      },
      "source": [
        "model = Model(inputs=[input_tokens, input_segments],\n",
        "              outputs=[out])\n",
        "model.compile(optimizer=\"adam\", loss=[\"categorical_crossentropy\"], metrics=[\"accuracy\"])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNt36S7XjYDY",
        "outputId": "93b38c26-5044-4960-da10-fe178d1a9ca7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Functional)            (None, 128, 768)     93116328    input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 128, 768)     0           model_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 128, 256)     196864      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "k_max_pooling (KMaxPooling)     (None, 4, 256)       0           time_distributed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1024)         0           k_max_pooling[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4)            4100        flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 4)            0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 93,317,292\n",
            "Trainable params: 514,476\n",
            "Non-trainable params: 92,802,816\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_xBUueCjYDZ"
      },
      "source": [
        "### Dataset loading functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHqKdeIajYDa"
      },
      "source": [
        "def iter_training_sample(fn):\n",
        "    ret = []\n",
        "    for seq_record in SeqIO.parse(fn, \"fasta\"):\n",
        "        struct = seq_record.description.split()[1]\n",
        "        seq = str(seq_record.seq)\n",
        "        ret.append((seq.upper(), struct[0]))\n",
        "    random.seed(fn)\n",
        "    random.shuffle(ret)\n",
        "    return ret\n",
        "\n",
        "def train_gen(data, batch_size, shuffle=True, enable_sampling=True):\n",
        "    while True:\n",
        "        data_it = data[:]\n",
        "        if shuffle:\n",
        "            random.shuffle(data_it)\n",
        "        for idx0 in range(0, len(data_it), batch_size):\n",
        "            batch = data_it[idx0:idx0+batch_size]\n",
        "            yield vectorize_batch(batch, enable_sampling=enable_sampling)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s9AAEzGjYDa"
      },
      "source": [
        "train_data = iter_training_sample(TRAIN_DATA)\n",
        "train_iter = train_gen(train_data, BATCH_SIZE)\n",
        "TRAIN_STEPS = len(train_data) // BATCH_SIZE\n",
        "\n",
        "dev_data = iter_training_sample(DEV_DATA)\n",
        "dev_iter = train_gen(dev_data, BATCH_SIZE, enable_sampling=False)\n",
        "DEV_STEPS = len(dev_data) // BATCH_SIZE"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kphMdrFfjYDc"
      },
      "source": [
        "### Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVqll_HejYDd",
        "outputId": "1372f924-33da-41ed-ee63-e8a7687af727"
      },
      "source": [
        "callbacks = [LearningRateScheduler(lambda n: LR_START*1/((n+1)**0.5), verbose=1)]\n",
        "\n",
        "model.fit(train_iter, steps_per_epoch=TRAIN_STEPS, epochs=EPOCHS,\n",
        "          validation_data=dev_iter,\n",
        "          validation_steps=DEV_STEPS,\n",
        "          callbacks=callbacks)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
            "630/630 [==============================] - 444s 671ms/step - loss: 0.6662 - accuracy: 0.7799 - val_loss: 0.1416 - val_accuracy: 0.9489\n",
            "Epoch 2/3\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0007071067811865475.\n",
            "630/630 [==============================] - 418s 664ms/step - loss: 0.1421 - accuracy: 0.9496 - val_loss: 0.1200 - val_accuracy: 0.9664\n",
            "Epoch 3/3\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0005773502691896258.\n",
            "630/630 [==============================] - 418s 664ms/step - loss: 0.1108 - accuracy: 0.9616 - val_loss: 0.1037 - val_accuracy: 0.9674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff51987d978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzqdr3-LjYDe"
      },
      "source": [
        "### Predictions on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw1KH30ojYDe",
        "outputId": "a93de8de-d5e2-4358-d633-0092d2a4cb10"
      },
      "source": [
        "test_data = iter_training_sample(TEST_DATA)\n",
        "\n",
        "pred_Y = 0\n",
        "for i in range(PREDICT_AVG):\n",
        "    test_X, test_Y = next(train_gen(test_data, len(test_data), shuffle=False, enable_sampling=(i!=0)))\n",
        "    pred_Y = pred_Y + model.predict(test_X, batch_size=128, verbose=True)\n",
        "\n",
        "test_Y1 = test_Y.argmax(axis=1)\n",
        "pred_Y1 = pred_Y.argmax(axis=1)\n",
        "print(\"Model accuracy\", accuracy_score(test_Y1, pred_Y1))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40/40 [==============================] - 54s 1s/step\n",
            "40/40 [==============================] - 49s 1s/step\n",
            "40/40 [==============================] - 49s 1s/step\n",
            "40/40 [==============================] - 49s 1s/step\n",
            "40/40 [==============================] - 49s 1s/step\n",
            "Model accuracy 0.965639112507363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "jQjpAyYzjYDe",
        "outputId": "3f8f4405-82f5-4951-c97c-a8a10b373ba8"
      },
      "source": [
        "print(\"Confusion matrix\")\n",
        "pd.DataFrame(confusion_matrix(test_Y1, pred_Y1),\n",
        "             columns=list(TARGETS),\n",
        "             index=list(TARGETS)).round(2)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>d</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>862</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>b</th>\n",
              "      <td>1</td>\n",
              "      <td>1323</td>\n",
              "      <td>4</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>c</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1548</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>d</th>\n",
              "      <td>14</td>\n",
              "      <td>29</td>\n",
              "      <td>16</td>\n",
              "      <td>1185</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     a     b     c     d\n",
              "a  862     0     0    26\n",
              "b    1  1323     4    55\n",
              "c    5     3  1548    22\n",
              "d   14    29    16  1185"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eivpBhUjYDe",
        "outputId": "cbc04131-cf8a-4651-a9e3-03dfad48d253"
      },
      "source": [
        "test_data = iter_training_sample(TEST_DATA_NEW)\n",
        "\n",
        "pred_Y = 0\n",
        "for i in range(PREDICT_AVG):\n",
        "    test_X, test_Y = next(train_gen(test_data, len(test_data), shuffle=False, enable_sampling=(i!=0)))\n",
        "    pred_Y = pred_Y + model.predict(test_X, batch_size=128, verbose=True)\n",
        "\n",
        "test_Y1 = test_Y.argmax(axis=1)\n",
        "pred_Y1 = pred_Y.argmax(axis=1)\n",
        "print(\"Model accuracy\", accuracy_score(test_Y1, pred_Y1))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 6s 1s/step\n",
            "5/5 [==============================] - 6s 1s/step\n",
            "5/5 [==============================] - 6s 1s/step\n",
            "5/5 [==============================] - 6s 1s/step\n",
            "5/5 [==============================] - 6s 1s/step\n",
            "Model accuracy 0.9462540716612378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc0XQU45jYDf"
      },
      "source": [
        "model.save(SAVE_MODEL_FN)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDNYbAUlB4jw"
      },
      "source": [
        "## Copy out the resulting model into Google Drive\n",
        "\n",
        "Clink on the displayed link and authorize the app to access your google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXQumnWzCAzg",
        "outputId": "11c2ab3e-20a9-4186-e786-1c1ff5af43ef"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ofvWTI2HBFe"
      },
      "source": [
        "Modify the target directory according to your needs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo4sda_dCJB7"
      },
      "source": [
        "!cp $SAVE_MODEL_FN gdrive/MyDrive/Models/"
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}